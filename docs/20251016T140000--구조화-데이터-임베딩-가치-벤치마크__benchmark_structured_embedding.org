* 🎯 구조화 데이터 임베딩 가치 벤치마크: "최소 노력, 최대 효과"
:PROPERTIES:
:CUSTOM_ID: 구조화-데이터-임베딩-가치-벤치마크-최소-노력-최대-효과
:END:
*핵심 질문*: > "인간이 최소한의 노력으로 하는 구조화 (Denote, Org
heading, FILETAGS, 폴더)가 > 임베딩과 RAG 품질에 얼마나 기여하는가?"

*가설*:

#+begin_example
인간 친화적 구조화 = AI 친화적

Denote 파일명 (timestamp--한글제목__영어태그)
    → 파싱 가능, 메타데이터 풍부
    → 임베딩 품질 향상

Org heading (*, **, ***)
    → 계층 구조, 의미 단위
    → 청킹 품질 향상

FILETAGS (:tag1:tag2:)
    → 컨텍스트 명확
    → 검색 정확도 향상

폴더 구조 (meta, bib, journal, notes)
    → 지식 계층
    → RAG 답변 품질 향상
#+end_example

*측정 목표*:

#+begin_example
비구조화 (일반 파일명, flat text)
    vs
구조화 (Denote + Org + FILETAGS + 폴더)
    →
품질 차이 = 구조화의 가치!
#+end_example

--------------

** 📊 기존 임베딩 벤치마크 (MTEB/BEIR)
:PROPERTIES:
:CUSTOM_ID: 기존-임베딩-벤치마크-mtebbeir
:END:
*** MTEB (Massive Text Embedding Benchmark)
:PROPERTIES:
:CUSTOM_ID: mteb-massive-text-embedding-benchmark
:END:
*규모*:

#+begin_src yaml
Datasets: 58개
Languages: 112개
Tasks: 8가지
  - Retrieval (검색)
  - STS (의미 유사도)
  - Classification (분류)
  - Clustering (군집화)
  - Reranking (재순위)
  - Summarization (요약)
  - Bitext Mining (이중 텍스트)
  - Pair Classification (쌍 분류)
#+end_src

*Retrieval Metrics*:

#+begin_src python
nDCG@10 (Normalized Discounted Cumulative Gain)
MRR@10 (Mean Reciprocal Rank)
MAP@10 (Mean Average Precision)
Recall@10
Precision@10
#+end_src

*BEIR 데이터셋* (18개): - MS MARCO, TREC-COVID, Natural Questions -
HotpotQA, FiQA, ArguAna, Touché-2020 - CQADupStack, Quora, DBPedia,
SCIDOCS - FEVER, Climate-FEVER, SciFact, ...

--------------

*** RAG Evaluation Metrics
:PROPERTIES:
:CUSTOM_ID: rag-evaluation-metrics
:END:
*검색 품질* (Retrieval):

#+begin_src python
MRR (Mean Reciprocal Rank):
  # 정답이 몇 번째에 나오는가?
  # 1위: 1.0, 2위: 0.5, 3위: 0.33...
  # 빠른 정답 발견 측정

NDCG@k (Normalized DCG):
  # 순위별 가중치 (상위일수록 중요)
  # 다중 정답의 순위 품질

Recall@k:
  # 전체 정답 중 k개 안에 몇 개?
  # 정답 놓치지 않는지 측정

Precision@k:
  # k개 중 정답 비율
  # 노이즈 얼마나 적은지
#+end_src

*RAG 답변 품질* (Generation):

#+begin_src python
Faithfulness:
  # 답변이 검색된 문서에 기반했는가?

Answer Relevancy:
  # 답변이 질문과 관련 있는가?

Context Precision:
  # 검색된 컨텍스트가 정확한가?

Context Recall:
  # 필요한 컨텍스트를 다 가져왔는가?
#+end_src

--------------

** 🧪 구조화 가치 측정 벤치마크 설계
:PROPERTIES:
:CUSTOM_ID: 구조화-가치-측정-벤치마크-설계
:END:
*** 핵심 아이디어: "단계적 구조화"
:PROPERTIES:
:CUSTOM_ID: 핵심-아이디어-단계적-구조화
:END:
*비교군 설계* (A→E, 단계적 구조 추가):

#+begin_src yaml
A조 (비구조화):
  파일명: "doc001.md", "doc002.md", "doc003.md"
  내용: flat markdown (heading 없음)
  태그: 없음
  폴더: 단일 디렉토리

B조 (Denote 파일명만):
  파일명: "20250101T120000--머신러닝-개념__ml_ai.md"
  내용: flat markdown (동일)
  태그: 파일명에만 (ml, ai)
  폴더: 단일 디렉토리

C조 (+ Markdown Heading):
  파일명: Denote (B조와 동일)
  내용: # Heading, ## Subheading 구조화
  태그: 파일명 (B조와 동일)
  폴더: 단일 디렉토리

D조 (+ Frontmatter tags):
  파일명: Denote (B조와 동일)
  내용: Heading + Frontmatter
    ---
    title: 머신러닝 개념
    tags: ["ml", "ai", "머신러닝", "개념"]
    ---
  태그: 파일명 + Frontmatter (한글/영어)
  폴더: 단일 디렉토리

E조 (+ 폴더 구조):
  파일명: Denote (B조와 동일)
  내용: Heading + Frontmatter (D조와 동일)
  태그: 파일명 + Frontmatter (D조와 동일)
  폴더: meta/, bib/, journal/, notes/ (계층 구조)
#+end_src

*동일 내용, 구조만 차이!*

--------------

*** 테스트 데이터셋 구성
:PROPERTIES:
:CUSTOM_ID: 테스트-데이터셋-구성
:END:
*1. 소스 문서* (100개):

#+begin_src yaml
주제별 분포 (한글/영어 혼재):
  - 머신러닝/ML: 20개
  - Emacs/이맥스: 20개
  - NixOS/닉스: 20개
  - 지식관리/PKM: 20개
  - 기타: 20개

폴더 분포:
  - meta: 25개 (개념)
  - bib: 25개 (서지)
  - journal: 25개 (일일)
  - notes: 25개 (종합)

파일당 크기:
  - 평균: 500-1000 tokens
  - Heading: 2-5개/파일
  - Tags: 3-5개/파일
#+end_src

*2. 질문-답변 쌍* (50개):

#+begin_src yaml
질문 유형:
  1. 개념 질문 (20개):
     "머신러닝이란 무엇인가?"
     → meta 폴더에 답 있음

  2. 참고문헌 질문 (10개):
     "머신러닝 관련 추천 도서는?"
     → bib 폴더에 답 있음

  3. 경험 질문 (10개):
     "NixOS 설치 시 문제 해결 방법은?"
     → journal 폴더에 답 있음

  4. 통합 질문 (10개):
     "Emacs에서 머신러닝 활용 전체 워크플로우는?"
     → notes 폴더 + 여러 파일 통합

언어 분포:
  - 한글 쿼리: 25개
  - 영어 쿼리: 15개
  - 혼재 쿼리: 10개 ("Emacs 설정 방법")
#+end_src

*3. Ground Truth* (정답 문서):

#+begin_src python
# 각 질문마다 정답 문서 ID 목록
test_queries = {
    "q001": {
        "query": "머신러닝이란 무엇인가?",
        "lang": "ko",
        "relevant_docs": [
            "20250101T120000",  # meta/머신러닝-개념
            "20250102T130000"   # notes/ml-정리
        ],
        "expected_folder": "meta",  # 가장 관련 높은 폴더
        "min_mrr": 0.8  # 기대 MRR
    },
    # ... 50개
}
#+end_src

--------------

** 🧪 실험 프로토콜
:PROPERTIES:
:CUSTOM_ID: 실험-프로토콜
:END:
*** Step 1: 데이터 준비 (A-E 조별)
:PROPERTIES:
:CUSTOM_ID: step-1-데이터-준비-a-e-조별
:END:
#+begin_src python
# 동일한 100개 문서를 5가지 방식으로 가공

def create_dataset_variant(variant: str, docs: List[str]):
    """
    variant: 'A', 'B', 'C', 'D', 'E'
    """
    if variant == 'A':  # 비구조화
        for i, doc in enumerate(docs):
            filename = f"doc{i:03d}.md"
            content = strip_all_structure(doc)  # heading 제거, flat
            save(filename, content)

    elif variant == 'B':  # Denote 파일명
        for doc in docs:
            meta = extract_denote_meta(doc)
            filename = f"{meta['id']}--{meta['korean_title']}__{meta['tags']}.md"
            content = strip_all_structure(doc)  # flat
            save(filename, content)

    elif variant == 'C':  # + Heading
        for doc in docs:
            meta = extract_denote_meta(doc)
            filename = f"{meta['id']}--{meta['korean_title']}__{meta['tags']}.md"
            content = keep_headings(doc)  # heading 유지
            save(filename, content)

    elif variant == 'D':  # + Frontmatter
        for doc in docs:
            meta = extract_denote_meta(doc)
            filename = f"{meta['id']}--{meta['korean_title']}__{meta['tags']}.md"
            content = f"""---
title: {meta['korean_title']}
tags: {meta['all_tags']}  # 한글+영어
---

{keep_headings(doc)}
"""
            save(filename, content)

    elif variant == 'E':  # + 폴더 구조
        for doc in docs:
            meta = extract_denote_meta(doc)
            folder = meta['folder']  # meta, bib, journal, notes
            filename = f"{folder}/{meta['id']}--{meta['korean_title']}__{meta['tags']}.md"
            content = D조와 동일
            save(filename, content)
#+end_src

--------------

*** Step 2: 임베딩 (동일 모델, 동일 설정)
:PROPERTIES:
:CUSTOM_ID: step-2-임베딩-동일-모델-동일-설정
:END:
#+begin_src python
# 모든 조에 동일하게 적용

embedding_model = "mxbai-embed-large"  # 1024-dim, 한글 최적화
chunk_size = 800  # 기본값
overlap = 100

for variant in ['A', 'B', 'C', 'D', 'E']:
    # 파일 읽기
    files = load_files(variant)

    # 임베딩 텍스트 준비 (조별 차이!)
    for file in files:
        if variant == 'A':
            text_to_embed = file.content  # 내용만

        elif variant == 'B':
            denote = parse_denote_filename(file.name)
            text_to_embed = f"{denote['korean_title']} {' '.join(denote['tags'])} {file.content}"

        elif variant == 'C':
            denote = parse_denote_filename(file.name)
            headings = extract_headings(file.content)
            heading_text = ' '.join([h['text'] for h in headings])
            text_to_embed = f"{denote['korean_title']} {' '.join(denote['tags'])} {heading_text} {file.content}"

        elif variant == 'D':
            frontmatter = parse_frontmatter(file.content)
            denote = parse_denote_filename(file.name)
            headings = extract_headings(file.content)
            text_to_embed = f"{frontmatter['title']} {' '.join(frontmatter['tags'])} {heading_text} {file.content}"

        elif variant == 'E':
            # D조 + 폴더 정보
            folder = get_folder(file.path)
            text_to_embed = f"{folder} {D조와 동일}"

        # 임베딩
        embedding = model.embed(text_to_embed)
        save_to_vectordb(variant, file.id, embedding, metadata)
#+end_src

--------------

*** Step 3: 검색 평가 (50개 쿼리)
:PROPERTIES:
:CUSTOM_ID: step-3-검색-평가-50개-쿼리
:END:
#+begin_src python
# 각 조마다 동일한 50개 쿼리로 평가

results = {}

for variant in ['A', 'B', 'C', 'D', 'E']:
    vectordb = load_vectordb(variant)

    mrr_scores = []
    recall_at_5 = []
    recall_at_10 = []
    ndcg_scores = []

    for query_id, query_data in test_queries.items():
        query = query_data['query']
        relevant_docs = query_data['relevant_docs']

        # 검색
        search_results = vectordb.search(query, k=10)

        # MRR 계산
        for rank, result in enumerate(search_results, 1):
            if result['id'] in relevant_docs:
                mrr_scores.append(1.0 / rank)
                break

        # Recall@5, Recall@10
        top5_ids = [r['id'] for r in search_results[:5]]
        top10_ids = [r['id'] for r in search_results[:10]]

        recall_5 = len(set(top5_ids) & set(relevant_docs)) / len(relevant_docs)
        recall_10 = len(set(top10_ids) & set(relevant_docs)) / len(relevant_docs)

        recall_at_5.append(recall_5)
        recall_at_10.append(recall_10)

        # NDCG 계산
        ndcg = calculate_ndcg(search_results, relevant_docs, k=10)
        ndcg_scores.append(ndcg)

    # 결과 저장
    results[variant] = {
        'MRR@10': np.mean(mrr_scores),
        'Recall@5': np.mean(recall_at_5),
        'Recall@10': np.mean(recall_at_10),
        'NDCG@10': np.mean(ndcg_scores)
    }

# 비교표 생성
print_comparison_table(results)
#+end_src

--------------

*** Step 4: 예상 결과 (가설)
:PROPERTIES:
:CUSTOM_ID: step-4-예상-결과-가설
:END:
#+begin_src yaml
A조 (비구조화):
  MRR@10: 0.55
  Recall@5: 0.60
  Recall@10: 0.75
  NDCG@10: 0.62

B조 (Denote 파일명):
  MRR@10: 0.68 (+24%)
  Recall@5: 0.71 (+18%)
  Recall@10: 0.82 (+9%)
  NDCG@10: 0.71 (+15%)

C조 (+ Heading):
  MRR@10: 0.76 (+38%)
  Recall@5: 0.78 (+30%)
  Recall@10: 0.88 (+17%)
  NDCG@10: 0.78 (+26%)

D조 (+ Frontmatter):
  MRR@10: 0.82 (+49%)
  Recall@5: 0.83 (+38%)
  Recall@10: 0.91 (+21%)
  NDCG@10: 0.83 (+34%)

E조 (+ 폴더 구조):
  MRR@10: 0.88 (+60%)
  Recall@5: 0.87 (+45%)
  Recall@10: 0.94 (+25%)
  NDCG@10: 0.87 (+40%)

구조화 효과:
  A → E: MRR 60% 향상!
  A → B: 파일명만으로 24% 향상
  B → C: Heading 추가로 14% 향상
  C → D: Frontmatter 추가로 11% 향상
  D → E: 폴더 구조로 6% 향상
#+end_src

--------------

** 📐 벤치마크 테스트셋 구성
:PROPERTIES:
:CUSTOM_ID: 벤치마크-테스트셋-구성
:END:
*** 실제 데이터 활용
:PROPERTIES:
:CUSTOM_ID: 실제-데이터-활용
:END:
*소스*: ~/org/ 실제 파일 100개 샘플링

#+begin_src python
import random

def create_benchmark_dataset():
    """실제 Org 파일에서 벤치마크 생성"""

    # 1. ~/org/에서 100개 파일 샘플링
    folders = {
        'meta': 25,
        'bib': 25,
        'journal': 25,
        'notes': 25
    }

    sampled_files = []
    for folder, count in folders.items():
        folder_path = Path(f"~/org/{folder}")
        all_files = list(folder_path.glob("*.org"))

        # 파일 크기 기준 (500-1000 tokens)
        filtered = [f for f in all_files
                    if 500 < count_tokens(f.read_text()) < 1000]

        sampled = random.sample(filtered, count)
        sampled_files.extend([(folder, f) for f in sampled])

    # 2. Org → Markdown 변환
    for folder, org_file in sampled_files:
        # Denote 파일명 유지
        denote_meta = parse_denote_filename(org_file.name)

        # Org → Markdown 변환
        content_md = org_to_markdown(org_file.read_text())

        # Frontmatter 생성
        frontmatter = extract_org_frontmatter(org_file)

        # 저장 (E조 형태)
        save_as_variant_E(folder, denote_meta, content_md, frontmatter)

    # 3. E조에서 A-D조 파생
    create_variant_A_from_E()  # 구조 제거
    create_variant_B_from_E()  # 파일명만 유지
    create_variant_C_from_E()  # + Heading
    create_variant_D_from_E()  # + Frontmatter

    return sampled_files
#+end_src

--------------

*** 질문-답변 쌍 생성
:PROPERTIES:
:CUSTOM_ID: 질문-답변-쌍-생성
:END:
*자동 생성 + 수동 검증*:

#+begin_src python
def generate_qa_pairs(docs: List[str]):
    """LLM으로 질문 생성 후 수동 검증"""

    qa_pairs = []

    for doc in docs:
        # 1. 문서에서 핵심 개념 추출
        concepts = extract_concepts(doc)

        # 2. Claude/GPT로 질문 생성
        for concept in concepts:
            questions = llm_generate_questions(concept, doc)

            # 3. 수동 검증 필요
            for q in questions:
                qa_pairs.append({
                    'id': generate_id(),
                    'query': q['question'],
                    'relevant_docs': [doc.id],
                    'answer': q['answer'],
                    'folder': doc.folder,
                    'verified': False  # 수동 검증 필요
                })

    # 4. CSV 저장 → 수동 검증
    save_csv("qa_pairs_draft.csv", qa_pairs)

    # 5. 수동 검증 후
    verified = load_csv("qa_pairs_verified.csv")
    return verified
#+end_src

*한글 쿼리 특화*:

#+begin_src python
query_templates_korean = [
    "{개념}이란 무엇인가?",
    "{도구}를 사용하는 방법은?",
    "{문제} 해결 방법은?",
    "{주제}에 대해 설명해줘",
    "{저자}의 {책}에서 {개념}은?",
]

# 예시
"머신러닝이란 무엇인가?"
"Emacs를 사용하는 방법은?"
"NixOS 설치 오류 해결 방법은?"
#+end_src

--------------

** 📊 평가 지표 상세
:PROPERTIES:
:CUSTOM_ID: 평가-지표-상세
:END:
*** Metric 1: MRR (Mean Reciprocal Rank)
:PROPERTIES:
:CUSTOM_ID: metric-1-mrr-mean-reciprocal-rank
:END:
*계산*:

#+begin_src python
def calculate_mrr(search_results, relevant_docs):
    """
    정답이 몇 번째에 나오는가?

    1위: 1.0
    2위: 0.5
    3위: 0.333
    10위: 0.1
    없음: 0.0
    """
    for rank, result in enumerate(search_results, 1):
        if result['id'] in relevant_docs:
            return 1.0 / rank
    return 0.0

# 50개 쿼리 평균
mrr_scores = [calculate_mrr(search(q), relevant[q]) for q in queries]
MRR = np.mean(mrr_scores)
#+end_src

*해석*:

#+begin_example
MRR = 1.0: 모든 쿼리에서 1위에 정답
MRR = 0.5: 평균 2위에 정답
MRR = 0.33: 평균 3위에 정답

A조: 0.55 (평균 1.8위에 정답)
E조: 0.88 (평균 1.1위에 정답)

→ 구조화로 정답 순위 향상!
#+end_example

--------------

*** Metric 2: Recall@k
:PROPERTIES:
:CUSTOM_ID: metric-2-recallk
:END:
*계산*:

#+begin_src python
def calculate_recall_at_k(search_results, relevant_docs, k=5):
    """
    Top-k에 정답이 몇 개나 포함되었는가?

    relevant_docs = [doc1, doc2, doc3]  # 3개 정답
    top5 = [doc1, doc4, doc5, doc6, doc7]  # 1개만 포함

    Recall@5 = 1/3 = 0.33
    """
    top_k_ids = [r['id'] for r in search_results[:k]]
    found = len(set(top_k_ids) & set(relevant_docs))
    total = len(relevant_docs)

    return found / total if total > 0 else 0
#+end_src

*해석*:

#+begin_example
Recall@5 = 1.0: 정답 전부 Top-5 안에
Recall@5 = 0.67: 정답 3개 중 2개만 Top-5

A조: Recall@5 = 0.60 (정답 놓침)
E조: Recall@5 = 0.87 (대부분 찾음)

→ 구조화로 정답 회수율 향상!
#+end_example

--------------

*** Metric 3: NDCG@k (Normalized DCG)
:PROPERTIES:
:CUSTOM_ID: metric-3-ndcgk-normalized-dcg
:END:
*계산*:

#+begin_src python
def calculate_ndcg_at_k(search_results, relevant_docs, k=10):
    """
    순위별 가중치 적용

    1위: 가중치 높음 (1.0 / log2(2) = 1.0)
    2위: 중간 (1.0 / log2(3) = 0.63)
    10위: 낮음 (1.0 / log2(11) = 0.29)
    """
    import math

    # DCG (Discounted Cumulative Gain)
    dcg = 0.0
    for rank, result in enumerate(search_results[:k], 1):
        if result['id'] in relevant_docs:
            relevance = 1.0  # binary (관련 or 무관)
            dcg += relevance / math.log2(rank + 1)

    # IDCG (Ideal DCG) - 모든 정답이 상위에 있을 때
    idcg = sum(1.0 / math.log2(i + 2) for i in range(len(relevant_docs)))

    # Normalize
    return dcg / idcg if idcg > 0 else 0.0
#+end_src

*해석*:

#+begin_example
NDCG@10 = 1.0: 정답이 모두 상위에 (이상적)
NDCG@10 = 0.8: 정답 대부분 상위에
NDCG@10 = 0.5: 정답이 뒤쪽에 분산

A조: 0.62 (정답이 분산됨)
E조: 0.87 (정답이 상위 집중)

→ 구조화로 정답 순위 품질 향상!
#+end_example

--------------

** 🎯 구조화 가치 측정 (핵심!)
:PROPERTIES:
:CUSTOM_ID: 구조화-가치-측정-핵심
:END:
*** "숨은그림찾기" 비유
:PROPERTIES:
:CUSTOM_ID: 숨은그림찾기-비유
:END:
*설정*:

#+begin_example
100개 그림 (문서)
50개 질문 ("빨간 사과는 어디?")
정답: 각 질문마다 관련 그림 ID

비구조화 (A조):
  그림 이름: pic001, pic002, pic003
  배치: 무작위
  힌트: 없음

  → 전부 뒤져야 함
  → 평균 1.8번째에 발견

구조화 (E조):
  그림 이름: timestamp--빨간-사과__과일_색깔.jpg
  배치: 폴더별 (과일/, 채소/, 동물/, 도구/)
  힌트: 파일명, 폴더, 태그

  → 과일/ 폴더만 검색
  → 파일명에서 "빨간-사과" 확인
  → 평균 1.1번째에 발견

개선: 64% 빠름 (1.8 → 1.1)
#+end_example

--------------

*** 구조 요소별 기여도 분해
:PROPERTIES:
:CUSTOM_ID: 구조-요소별-기여도-분해
:END:
#+begin_src yaml
구조 요소별 MRR 향상:

A → B (Denote 파일명):
  +0.13 (24% 향상)
  기여: 한글 제목 + 영어 태그

B → C (Heading):
  +0.08 (12% 향상)
  기여: 계층 구조 (의미 단위)

C → D (Frontmatter):
  +0.06 (8% 향상)
  기여: 한글 태그 추가, 메타데이터

D → E (폴더 구조):
  +0.06 (7% 향상)
  기여: 지식 계층 (meta → bib → journal → notes)

총 향상: A(0.55) → E(0.88) = +0.33 (60%)

결론:
  - Denote 파일명이 가장 큰 기여 (24%)
  - Heading이 두 번째 (12%)
  - Frontmatter, 폴더는 추가 개선 (각 7-8%)
#+end_src

--------------

** 🧠 "최소 노력" 정량화
:PROPERTIES:
:CUSTOM_ID: 최소-노력-정량화
:END:
*** 인간 노력 vs AI 이득
:PROPERTIES:
:CUSTOM_ID: 인간-노력-vs-ai-이득
:END:
#+begin_src yaml
A조 (비구조화) - 인간 노력:
  파일명: 자동 (doc001)
  내용: 그냥 작성 (flat)
  태그: 없음
  폴더: 단일

  인간 노력: 0시간 (기본)
  AI 품질: MRR 0.55

E조 (구조화) - 인간 노력:
  파일명: Denote 작성 (1분/파일)
  내용: Heading 구조화 (2분/파일)
  태그: Frontmatter 작성 (1분/파일)
  폴더: 분류 (30초/파일)

  인간 노력: 4.5분/파일 × 100 = 7.5시간
  AI 품질: MRR 0.88

ROI (투자 대비 효과):
  7.5시간 투자 → 60% 품질 향상

  시간당 품질 향상: 8% / hour
#+end_src

*하지만!*:

#+begin_example
인간 부수 효과 (측정 불가):
  ✅ Denote 파일명: 인간도 빠르게 찾음
  ✅ Heading: 인간도 구조 파악 쉬움
  ✅ Tags: 인간도 분류 편함
  ✅ 폴더: 인간도 관리 편함

  → AI만이 아니라 인간도 이득!
  → 투자 가치 100% 이상!
#+end_example

--------------

** 🔬 실험 디자인
:PROPERTIES:
:CUSTOM_ID: 실험-디자인
:END:
*** 테스트셋 구성
:PROPERTIES:
:CUSTOM_ID: 테스트셋-구성
:END:
*1. 100개 문서 준비*:

#+begin_src sh
# ~/org/에서 샘플링
python scripts/benchmark/sample_org_files.py \
  --source ~/org/ \
  --output ./benchmark/source_docs/ \
  --count 100 \
  --min-tokens 500 \
  --max-tokens 1000 \
  --folders meta:25,bib:25,journal:25,notes:25

# Org → Markdown 변환
python scripts/benchmark/org_to_markdown.py \
  --input ./benchmark/source_docs/ \
  --output ./benchmark/markdown/
#+end_src

*2. 5가지 변형 생성*:

#+begin_src sh
# A: 비구조화
python scripts/benchmark/create_variant_A.py \
  --input ./benchmark/markdown/ \
  --output ./benchmark/variant_A/

# B: Denote 파일명
python scripts/benchmark/create_variant_B.py \
  --input ./benchmark/markdown/ \
  --output ./benchmark/variant_B/

# C, D, E도 마찬가지
#+end_src

*3. 질문 생성*:

#+begin_src sh
# LLM으로 초안 생성
python scripts/benchmark/generate_questions.py \
  --docs ./benchmark/source_docs/ \
  --output ./benchmark/questions_draft.csv \
  --count 50 \
  --model claude-sonnet-4

# 수동 검증
# vi ./benchmark/questions_draft.csv
# → ./benchmark/questions_verified.csv
#+end_src

*4. 임베딩 & 평가*:

#+begin_src sh
# 각 변형 임베딩
for variant in A B C D E; do
  python scripts/benchmark/embed_variant.py \
    --input ./benchmark/variant_$variant/ \
    --db ./benchmark/vectordb_$variant.db \
    --model mxbai-embed-large
done

# 평가
python scripts/benchmark/evaluate_all.py \
  --questions ./benchmark/questions_verified.csv \
  --variants A B C D E \
  --output ./benchmark/results.csv
#+end_src

--------------

** 📈 결과 분석 프레임워크
:PROPERTIES:
:CUSTOM_ID: 결과-분석-프레임워크
:END:
*** 1. 정량적 비교
:PROPERTIES:
:CUSTOM_ID: 정량적-비교
:END:
#+begin_src python
# results.csv
variant,MRR@10,Recall@5,Recall@10,NDCG@10,Latency_ms
A,0.55,0.60,0.75,0.62,180
B,0.68,0.71,0.82,0.71,185
C,0.76,0.78,0.88,0.78,190
D,0.82,0.83,0.91,0.83,195
E,0.88,0.87,0.94,0.87,200

# 시각화
import matplotlib.pyplot as plt

metrics = ['MRR@10', 'Recall@5', 'Recall@10', 'NDCG@10']
variants = ['A', 'B', 'C', 'D', 'E']

for metric in metrics:
    plt.plot(variants, results[metric], marker='o')
    plt.title(f'{metric} by Structuring Level')
    plt.xlabel('Variant (A=None → E=Full)')
    plt.ylabel(metric)
    plt.savefig(f'benchmark_{metric}.png')
#+end_src

--------------

*** 2. 질적 분석
:PROPERTIES:
:CUSTOM_ID: 질적-분석
:END:
*쿼리별 상세 분석*:

#+begin_src python
# 쿼리별 결과 비교

query = "머신러닝 최적화 방법은?"
relevant_docs = ["20250101T120000", "20250203T140000"]

results_A = search_variant_A(query, k=10)
# Top 3: doc045, doc012, doc089 (정답 5위, MRR=0.2)

results_E = search_variant_E(query, k=10)
# Top 3:
#   1위: 20250101T120000--머신러닝-최적화__ml_optimization (정답!)
#   2위: 20250203T140000--신경망-학습__nn_training (정답!)
#   3위: 관련 문서
# MRR = 1.0

분석:
  A조: 정답 5위 (파일명 doc045는 의미 없음)
  E조: 정답 1-2위 (Denote 파일명에 "최적화" 포함)

  → Denote 파일명이 검색 품질 결정적 영향!
#+end_src

--------------

*** 3. 구조 요소별 기여도
:PROPERTIES:
:CUSTOM_ID: 구조-요소별-기여도
:END:
#+begin_src python
# Ablation Study (제거 실험)

baseline = E조  # 전체 구조

E_without_filename = {
    'filenames': 'doc001.md' (Denote 제거),
    '나머지': E조와 동일
}

E_without_heading = {
    'filenames': Denote (유지),
    'content': flat text (heading 제거),
    '나머지': E조와 동일
}

E_without_frontmatter = {
    'filenames': Denote (유지),
    'headings': 유지,
    'frontmatter': 제거,
    '나머지': E조와 동일
}

E_without_folder = {
    'filenames': Denote (유지),
    'headings': 유지,
    'frontmatter': 유지,
    'folder': 단일 디렉토리
}

# 각각 평가
결과:
  E (전체): MRR 0.88
  - filename: MRR 0.64 (-27%) ← 가장 중요!
  - heading: MRR 0.80 (-9%)
  - frontmatter: MRR 0.82 (-7%)
  - folder: MRR 0.82 (-7%)

기여도 순위:
  1. Denote 파일명 (27%)
  2. Heading (9%)
  3. Frontmatter (7%)
  4. 폴더 구조 (7%)
#+end_src

--------------

** 📁 벤치마크 디렉토리 구조
:PROPERTIES:
:CUSTOM_ID: 벤치마크-디렉토리-구조
:END:
#+begin_example
~/repos/gh/memex-kb-benchmark/
├── source_docs/              # 원본 100개 Org 파일
│   ├── meta/
│   ├── bib/
│   ├── journal/
│   └── notes/
├── markdown/                 # Org → Markdown 변환
├── variants/
│   ├── A_unstructured/       # 비구조화
│   ├── B_denote_only/        # Denote 파일명
│   ├── C_with_headings/      # + Heading
│   ├── D_with_frontmatter/   # + Frontmatter
│   └── E_full_structure/     # + 폴더
├── vectordb/
│   ├── variant_A.db          # SQLite 또는 Supabase
│   ├── variant_B.db
│   ├── variant_C.db
│   ├── variant_D.db
│   └── variant_E.db
├── questions/
│   ├── questions_draft.csv   # LLM 생성
│   └── questions_verified.csv # 수동 검증
├── results/
│   ├── results.csv           # MRR, Recall, NDCG
│   ├── results_by_query.csv  # 쿼리별 상세
│   ├── ablation_study.csv    # 요소별 기여도
│   └── visualizations/       # 그래프
├── scripts/
│   ├── sample_org_files.py
│   ├── org_to_markdown.py
│   ├── create_variant_*.py
│   ├── embed_variant.py
│   └── evaluate_all.py
└── README.md                 # 벤치마크 문서
#+end_example

--------------

** 🚀 실행 계획 (2주)
:PROPERTIES:
:CUSTOM_ID: 실행-계획-2주
:END:
*** Week 1: 데이터 준비
:PROPERTIES:
:CUSTOM_ID: week-1-데이터-준비
:END:
#+begin_example
Day 1-2: 샘플링 & 변환
  - [ ] ~/org/에서 100개 파일 샘플링
  - [ ] Org → Markdown 변환
  - [ ] 5가지 변형 생성 (A-E)

Day 3-5: 질문 생성
  - [ ] Claude로 질문 50개 초안 생성
  - [ ] 수동 검증 (정답 문서 ID 확인)
  - [ ] 한글/영어/혼재 분포 확인

Day 6-7: 임베딩
  - [ ] 5가지 변형 각각 임베딩
  - [ ] mxbai-embed-large (1024-dim)
  - [ ] Supabase 또는 SQLite
#+end_example

*** Week 2: 평가 & 분석
:PROPERTIES:
:CUSTOM_ID: week-2-평가-분석
:END:
#+begin_example
Day 1-2: 검색 평가
  - [ ] 50개 쿼리 × 5 variants = 250회 검색
  - [ ] MRR, Recall@5, Recall@10, NDCG@10 계산

Day 3-4: 상세 분석
  - [ ] 구조 요소별 기여도 (Ablation)
  - [ ] 쿼리별 상세 분석
  - [ ] 실패 사례 분석

Day 5-7: 문서화 & 공개
  - [ ] 벤치마크 결과 보고서
  - [ ] 시각화 (그래프)
  - [ ] GitHub 공개 (memex-kb-benchmark)
  - [ ] 논문/블로그 작성 (선택)
#+end_example

--------------

** 💡 독창적 기여
:PROPERTIES:
:CUSTOM_ID: 독창적-기여
:END:
*** 기존 벤치마크 vs memex-kb 벤치마크
:PROPERTIES:
:CUSTOM_ID: 기존-벤치마크-vs-memex-kb-벤치마크
:END:
*MTEB/BEIR* (기존):

#+begin_example
목적: 임베딩 모델 성능 비교
데이터: 영어 중심, 범용 문서
평가: 모델 A vs 모델 B

초점: 모델 자체의 성능
#+end_example

*memex-kb 벤치마크* (신규):

#+begin_example
목적: 구조화의 가치 측정
데이터: 한글/영어 혼재, 개인 지식베이스
평가: 구조화 A조 vs E조

초점: 인간의 구조화 노력이 AI에 미치는 영향!
#+end_example

*차별화*:

#+begin_example
1. "구조화의 가치" 측정
   → 기존 연구 없음!

2. Denote 철학 검증
   → "file-naming scheme matters!"

3. 한글 환경 특화
   → MTEB는 영어 중심

4. 최소 노력 정량화
   → 시간당 품질 향상

5. 실용적 가이드
   → "이렇게 구조화하면 이만큼 좋아짐"
#+end_example

--------------

** 📚 한글 RAG 벤치마크 데이터셋 활용
:PROPERTIES:
:CUSTOM_ID: 한글-rag-벤치마크-데이터셋-활용
:END:
*** allganize/RAG-Evaluation-Dataset-KO
:PROPERTIES:
:CUSTOM_ID: allganizerag-evaluation-dataset-ko
:END:
*Hugging Face*:
https://huggingface.co/datasets/allganize/RAG-Evaluation-Dataset-KO

*구성*:

#+begin_src yaml
도메인: 5개
  - finance (금융)
  - public (공공)
  - healthcare (의료)
  - legal (법률)
  - commerce (상업)

문서: 200-300 페이지/도메인
질문: 각 도메인별 다수
평가: 자동 (생성 답변 vs 정답)
#+end_src

*활용 방안*:

#+begin_example
1. 한글 쿼리 참고
   - 도메인별 질문 패턴 학습
   - 한글 표현 방식

2. 평가 방법론 참고
   - 자동 평가 기법
   - LLM judge 패턴

3. 비교 기준
   - memex-kb vs allganize
   - 개인 KB vs 엔터프라이즈
#+end_example

--------------

** 🎯 벤치마크 공개 전략
:PROPERTIES:
:CUSTOM_ID: 벤치마크-공개-전략
:END:
*** GitHub Repository: memex-kb-benchmark
:PROPERTIES:
:CUSTOM_ID: github-repository-memex-kb-benchmark
:END:
*구조*:

#+begin_example
memex-kb-benchmark/
├── README.md                 # "구조화의 가치" 논문 형식
├── data/
│   ├── source/               # 100개 원본 (공개 가능한 것만)
│   ├── variants/             # A-E 변형
│   └── questions.csv         # 50개 QA 쌍
├── results/
│   ├── benchmark_results.csv
│   ├── ablation_study.csv
│   └── visualizations/
├── scripts/
│   ├── create_variants.py
│   ├── embed_and_search.py
│   └── evaluate.py
└── docs/
    ├── METHODOLOGY.md        # 실험 설계
    ├── RESULTS.md            # 결과 분석
    └── INSIGHTS.md           # 통찰 및 가이드
#+end_example

*README.md 스토리*:

#+begin_src markdown
# Memex-KB Benchmark: Measuring the Value of Human Structuring

## Abstract

We measure the impact of human-friendly structuring (Denote filenames,
Markdown headings, frontmatter tags, folder hierarchy) on RAG quality.

Results:
- Denote filenames: +24% MRR improvement
- Markdown headings: +12% additional
- Frontmatter tags: +8% additional
- Folder structure: +7% additional
- **Total: +60% MRR improvement**

Conclusion:
,**Minimal human effort (4.5 min/doc) yields significant AI benefits.**

## Implications

"인간 친화적 구조 = AI 친화적 구조"

The same structure that helps humans organize and find information
also helps AI systems retrieve and generate better answers.

→ Win-Win for Human-AI collaboration!
#+end_src

--------------

** 🔗 관련 프로젝트
:PROPERTIES:
:CUSTOM_ID: 관련-프로젝트
:END:
*memex-kb*: - GitHub: https://github.com/junghan0611/memex-kb - 구조화
철학의 구현체

*embedding-config*: - 2,945개 Org 파일 임베딩 (검증됨) - 폴더별 차별화
전략

*참고 벤치마크*: - MTEB: https://github.com/embeddings-benchmark/mteb -
BEIR: https://github.com/beir-cellar/beir - allganize RAG-KO:
https://huggingface.co/datasets/allganize/RAG-Evaluation-Dataset-KO

--------------

** 📝 결론: "구조화는 투자다"
:PROPERTIES:
:CUSTOM_ID: 결론-구조화는-투자다
:END:
*** "최소 노력, 최대 효과"
:PROPERTIES:
:CUSTOM_ID: 최소-노력-최대-효과
:END:
#+begin_src yaml
인간 투자:
  Denote 파일명: 1분/파일
  Heading 구조: 2분/파일
  Frontmatter: 1분/파일
  폴더 분류: 0.5분/파일

  총: 4.5분/파일

AI 이득:
  MRR: +60% (0.55 → 0.88)
  Recall@5: +45%
  NDCG@10: +40%

  인간도 이득:
    ✅ 빠른 파일 찾기
    ✅ 구조 파악 쉬움
    ✅ 관리 편함

ROI: 무한대!
  (인간도 이득 + AI도 이득)
#+end_src

--------------

*** 다음 단계
:PROPERTIES:
:CUSTOM_ID: 다음-단계
:END:
*즉시*: 1. ~/org/에서 100개 파일 샘플링 2. 5가지 변형 생성 3. 질문 50개
생성

*2주 후*: 1. 벤치마크 결과 공개 2. "구조화의 가치" 논문/블로그 3. Denote
커뮤니티 공유

*장기*: 1. 한글 RAG 표준 벤치마크로 발전 2. MTEB-KO (Korean) 기여 3.
Emacs/Denote 커뮤니티 피드백

--------------

*최종 업데이트*: 2025-10-16T14:00:00+09:00 *다음*: 벤치마크 데이터셋
구성 시작

--------------

*"인간이 최소한의 노력으로 하는 구조화가 임베딩과 에이전트에 가치를
준다"* *"이를 측정하여 증명한다"* *"Denote 철학의 정량적 검증"*
